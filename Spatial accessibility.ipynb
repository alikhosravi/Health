{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAz6JpHuQ3mvrZWVzNWpiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alikhosravi/Spatial-Accessibility/blob/main/Spatial%20accessibility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåè Install packages\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H1TMsv7TRBQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!pip install mapclassify\n",
        "!pip install fitter\n",
        "!pip install libpysal"
      ],
      "metadata": {
        "id": "SrYKqFvQRFFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚≠ê Import libraries"
      ],
      "metadata": {
        "id": "1f4uJmHyRF6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import geopandas\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import robust_scale, minmax_scale\n",
        "from libpysal.weights import KNN\n",
        "import matplotlib\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.lines import Line2D\n",
        "drive.mount('/content/drive')\n",
        "db = gpd.read_file('/content/drive/My Drive/Isfahan/Inputs/MyBlocks.gpkg')\n",
        "variables=['T2H', 'T2L', 'T2C', 'T2P']"
      ],
      "metadata": {
        "id": "xvO96XLOQy-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öñ Remove Outliers"
      ],
      "metadata": {
        "id": "bnakOjzwRc1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Opimaum_Zscore=dict()\n",
        "for var in variables:\n",
        "  df = pandas.DataFrame(list(db[var]), columns=[\"Data\"]) \n",
        "  for col in df.columns:\n",
        "      col_zscore = col + \"_zscore\"\n",
        "      df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
        "\n",
        "  def outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.2, max_hist = None, bins = 50):\n",
        "      fig = plt.figure(figsize=(20, 6))\n",
        "      fig.suptitle(col, fontsize=16)\n",
        "      plt.subplot(1,3,1)\n",
        "      if max_hist == None:\n",
        "          sns.histplot(df[col], kde=False, bins = 50,color=\"r\")\n",
        "      else :\n",
        "          sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n",
        "      plt.subplot(1,3,2)\n",
        "      sns.boxplot(df[col])\n",
        "      plt.subplot(1,3,3)\n",
        "      z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n",
        "      global Oz\n",
        "      Oz= z_score_inspect[2]\n",
        "      return Oz\n",
        "      plt.show()\n",
        "\n",
        "  def outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n",
        "      z_scores = df[\"Data_zscore\"]\n",
        "      threshold_list = []\n",
        "      for threshold in np.arange(min_z, max_z, step):\n",
        "          threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n",
        "          df_outlier = pandas.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n",
        "          df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))/df_outlier.outlier_count*100\n",
        "      plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n",
        "      best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n",
        "      outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n",
        "      percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n",
        "      plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n",
        "                colors=\"r\", ls = \":\"\n",
        "                )\n",
        "      plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n",
        "                                                                    (np.round(percentile_threshold, 3), \n",
        "                                                                      np.round(100-percentile_threshold, 3))), \n",
        "                  (best_treshold, df_outlier.outlier_count.max()/2))\n",
        "      #plt.show()\n",
        "      if print_list:\n",
        "          print(df_outlier)\n",
        "      return (plt, df_outlier, best_treshold, outlier_limit, percentile_threshold)\n",
        "  outlier_inspect(df,\"Data\")\n",
        "  Opimaum_Zscore[var] = Oz\n",
        "\n",
        "for variable in variables:\n",
        "  db=db[abs(stats.zscore(db[variable]))<Opimaum_Zscore[variable]]"
      ],
      "metadata": {
        "id": "rI33F-sURNFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Power Transformation"
      ],
      "metadata": {
        "id": "apmsJh5pRoas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transformed_db = db.loc[:,tuple(variables)]\n",
        "for variable in variables:\n",
        "  data = list(db[variable]);\n",
        "  data = np.array(data);\n",
        "  pt = PowerTransformer()\n",
        "  pt.fit(data.reshape(-1,1));\n",
        "  mm = MinMaxScaler()\n",
        "  mm.fit(pt.transform(data.reshape(-1,1)))\n",
        "  Transformed_db[variable] = 1-mm.transform(pt.transform(data.reshape(-1,1)))"
      ],
      "metadata": {
        "id": "21EJ5qYLRlDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° Entropy weighting"
      ],
      "metadata": {
        "id": "cVJ5IC_FRvkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Entropy(labels, base=len(Transformed_db)):\n",
        "  value,counts = np.unique(labels, return_counts=True)\n",
        "  return entropy(counts, base=base)\n",
        "entropies=dict()\n",
        "for variable in Transformed_db[variables]:\n",
        "  entropies[variable]=1-Entropy(Transformed_db[variable])\n",
        "weights=dict()\n",
        "for variable in entropies:\n",
        "  weights[variable] = entropies[variable]/sum(entropies.values())"
      ],
      "metadata": {
        "id": "d2ebnqW-Ru6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí¢ Clustering\n"
      ],
      "metadata": {
        "id": "ks8aGl8jSNZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Entropy(labels, base=len(Transformed_db)):\n",
        "  value,counts = np.unique(labels, return_counts=True)\n",
        "  return entropy(counts, base=base)\n",
        "entropies=dict()\n",
        "for variable in Transformed_db[variables]:\n",
        "  entropies[variable]=1-Entropy(Transformed_db[variable])\n",
        "weights=dict()\n",
        "for variable in entropies:\n",
        "  weights[variable] = entropies[variable]/sum(entropies.values())\n",
        "\n",
        "Weighted_db = pandas.DataFrame()\n",
        "for variable in variables:\n",
        "  Weighted_db[variable] = Transformed_db[variable]*weights[variable]\n",
        "\n",
        "def Att(method):\n",
        "  global Attributes\n",
        "  Means_sum = [0]*5\n",
        "  for variable in variables:\n",
        "    Means_sum=[int(x + y) for x, y in zip(Means_sum, list(db.groupby(method)[variable].mean()))]\n",
        "  sorted_clusters=[]\n",
        "  for dis in Means_sum:\n",
        "    sorted_clusters.append(sorted(Means_sum, reverse= True).index(dis))\n",
        "  return sorted_clusters\n",
        "\n",
        "kmeans = KMeans(n_clusters=5)\n",
        "AP = np.array(AHP);\n",
        "k5cls = kmeans.fit(AP.reshape(-1,1))\n",
        "db['KNN'] = k5cls.labels_\n",
        "\n",
        "atr=Att('KNN')\n",
        "def adjust(r , sr):\n",
        "  return sr[r]\n",
        "db['KNN'] = db.apply(lambda row : adjust(row['KNN'], atr), axis = 1)\n",
        "\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "model = AgglomerativeClustering(n_clusters=5)\n",
        "yhat = model.fit(AP.reshape(-1,1))\n",
        "db['AGG'] = yhat.labels_\n",
        "atr2=Att('AGG')\n",
        "def adjust(r , sr):\n",
        "  return sr[r]\n",
        "db['AGG'] = db.apply(lambda row : adjust(row['AGG'], atr2), axis = 1)\n",
        "\n",
        "bis = pandas.read_csv('bisect.csv')\n",
        "db['BIS']=list(bis['0'])\n",
        "atr2=Att('BIS')\n",
        "def adjust(r , sr):\n",
        "  return sr[r]\n",
        "db['BIS'] = db.apply(lambda row : adjust(row['BIS'], atr2), axis = 1)"
      ],
      "metadata": {
        "id": "AzmqGhfwSYgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü•á Supervised Learning"
      ],
      "metadata": {
        "id": "U8sT2g31SuUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from pandas import read_csv\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "id": "BarBcYMzSy3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Superdb=db.loc[:,['D2H', 'D2L', 'D2C', 'D2P', 'Diff','geometry']]\n",
        "Superdb= Superdb.loc[Superdb['Diff'] != 'Different values']\n",
        "def stringer(x):\n",
        "  return str(x)\n",
        "Superdb['Diff'] = Superdb.apply(lambda row : stringer(row['Diff']), axis = 1)\n"
      ],
      "metadata": {
        "id": "1_JcqHgkS283"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split-out validation dataset\n",
        "array = DF.values\n",
        "X = array[:,0:4]\n",
        "y = array[:,4]\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "\n",
        "# Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(gamma='auto')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "\tkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "\tresults.append(cv_results)\n",
        "\tnames.append(name)\n",
        "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
      ],
      "metadata": {
        "id": "jPanQTptS5ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Superdb = Superdb.drop(columns=['geometry'])\n",
        "grouped = Superdb.groupby(Superdb['Diff'])\n",
        "\n",
        "DF = pandas.DataFrame()\n",
        "for cl in list(Superdb['Diff'].unique()):\n",
        "  df_new = grouped.get_group(cl)\n",
        "  df_new = df_new.iloc[:Superdb.groupby('Diff').size().min(),:]\n",
        "  DF = DF.append(df_new)\n",
        "\n",
        "X = DF\n",
        "array = X.values\n",
        "\n",
        "X = array[:,0:4]\n",
        "y = array[:,4]\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=2)\n",
        "model.fit(X_train, Y_train)\n",
        "predictions = model.predict(X_validation)\n",
        "\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))\n",
        "\n",
        "# Compare Algorithms\n",
        "pyplot.boxplot(results, labels=names)\n",
        "pyplot.title('Algorithm Comparison')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "3fKVnMBGS7OD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
